{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"daxt3MvkQnA0"},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspaces/Computer-vision-and-AI-ex-3/assignment3/CV7062610/datasets\n","/workspaces/Computer-vision-and-AI-ex-3/assignment3\n"]}],"source":["# this mounts your Google Drive to the Colab VM.\n","#from google.colab import drive\n","#drive.mount('/content/drive', force_remount=True)\n","\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'CV7062610/assignments/assignment3/'\n","FOLDERNAME = \"assignment3/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/workspaces/Computer-vision-and-AI-ex-3/assignment3')\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist.\n","#%cd drive/My\\ Drive/$FOLDERNAME/CV7062610/datasets/\n","%cd CV7062610/datasets/\n","#!bash get_datasets.sh\n","!bash get_datasets.sh\n","#%cd /content\n","%cd /workspaces/Computer-vision-and-AI-ex-3/assignment3"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"t7PGzj8MQnA8"},"outputs":[{"name":"stdout","output_type":"stream","text":["=========== You can safely ignore the message below if you are NOT working on ConvolutionalNetworks.ipynb ===========\n","\tYou will need to compile a Cython extension for a portion of this assignment.\n","\tThe instructions to do this will be given in a section of the notebook below.\n","\tThere will be an option for Colab users and another for Jupyter (local) users.\n"]}],"source":["# As usual, a bit of setup\n","from __future__ import print_function\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from CV7062610.classifiers.fc_net import *\n","from CV7062610.data_utils import get_CIFAR10_data\n","from CV7062610.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n","from CV7062610.solver import Solver\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","  \"\"\" returns relative error \"\"\"\n","  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfAnfI3_QnA8"},"outputs":[],"source":["# Load the (preprocessed) CIFAR10 data.\n","\n","data = get_CIFAR10_data()\n","for k, v in list(data.items()):\n","  print(('%s: ' % k, v.shape))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RmhAmxpUQnA9"},"source":["# Update rules\n","So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Y6O3ekPHQnA9"},"source":["# SGD+Momentum\n","Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent. See the Momentum Update section at http://cs231n.github.io/neural-networks-3/#sgd for more information.\n","\n","Open the file `CV7062610/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than e-8."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"iQrN56dxQnA-"},"outputs":[{"name":"stdout","output_type":"stream","text":["next_w error:  8.882347033505819e-09\n","velocity error:  4.269287743278663e-09\n"]}],"source":["from CV7062610.optim import sgd_momentum\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-3, 'velocity': v}\n","next_w, _ = sgd_momentum(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n","  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n","  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n","  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n","  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n","expected_velocity = np.asarray([\n","  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n","  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n","  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n","  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n","\n","# Should see relative errors around e-8 or less\n","print('next_w error: ', rel_error(next_w, expected_next_w))\n","print('velocity error: ', rel_error(expected_velocity, config['velocity']))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qZFwNSwaQnA-"},"source":["Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5IjiuYyeQnA_","scrolled":false},"outputs":[],"source":["num_train = 4000\n","small_data = {\n","  'X_train': data['X_train'][:num_train],\n","  'y_train': data['y_train'][:num_train],\n","  'X_val': data['X_val'],\n","  'y_val': data['y_val'],\n","}\n","\n","solvers = {}\n","\n","for update_rule in ['sgd', 'sgd_momentum']:\n","  print('running with ', update_rule)\n","  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule,\n","                  optim_config={\n","                    'learning_rate': 5e-3,\n","                  },\n","                  verbose=True)\n","  solvers[update_rule] = solver\n","  solver.train()\n","  print()\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","\n","for update_rule, solver in solvers.items():\n","  plt.subplot(3, 1, 1)\n","  plt.plot(solver.loss_history, 'o', label=\"loss_%s\" % update_rule)\n","  \n","  plt.subplot(3, 1, 2)\n","  plt.plot(solver.train_acc_history, '-o', label=\"train_acc_%s\" % update_rule)\n","\n","  plt.subplot(3, 1, 3)\n","  plt.plot(solver.val_acc_history, '-o', label=\"val_acc_%s\" % update_rule)\n","  \n","for i in [1, 2, 3]:\n","  plt.subplot(3, 1, i)\n","  plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vTdFbuJxQnBA"},"source":["# RMSProp and Adam\n","RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n","\n","In the file `CV7062610/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n","\n","**NOTE:** Please implement the _complete_ Adam update rule (with the bias correction mechanism), not the first simplified version mentioned in the course notes. \n","\n","[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n","\n","[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"lcIDdwZPQnBB"},"outputs":[{"name":"stdout","output_type":"stream","text":["next_w error:  9.524687511038133e-08\n","cache error:  2.6477955807156126e-09\n"]}],"source":["# Test RMSProp implementation\n","from CV7062610.optim import rmsprop\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'cache': cache}\n","next_w, _ = rmsprop(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n","  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n","  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n","  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n","  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n","expected_cache = np.asarray([\n","  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n","  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n","  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n","  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n","\n","# You should see relative errors around e-7 or less\n","print('next_w error: ', rel_error(expected_next_w, next_w))\n","print('cache error: ', rel_error(expected_cache, config['cache']))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"1ROQtzfIQnBB"},"outputs":[{"name":"stdout","output_type":"stream","text":["next_w error:  0.032064274004801614\n","v error:  4.208314038113071e-09\n","m error:  4.214963193114416e-09\n"]}],"source":["# Test Adam implementation\n","from CV7062610.optim import adam\n","\n","N, D = 4, 5\n","w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n","dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n","m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n","v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n","\n","config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n","next_w, _ = adam(w, dw, config=config)\n","\n","expected_next_w = np.asarray([\n","  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n","  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n","  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n","  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n","expected_v = np.asarray([\n","  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n","  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n","  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n","  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n","expected_m = np.asarray([\n","  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n","  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n","  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n","  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n","\n","# You should see relative errors around e-7 or less\n","print('next_w error: ', rel_error(expected_next_w, next_w))\n","print('v error: ', rel_error(expected_v, config['v']))\n","print('m error: ', rel_error(expected_m, config['m']))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"O54AifaOQnBC"},"source":["Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mp5XjhZpQnBC"},"outputs":[],"source":["learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n","for update_rule in ['adam', 'rmsprop']:\n","  print('running with ', update_rule)\n","  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n","\n","  solver = Solver(model, small_data,\n","                  num_epochs=5, batch_size=100,\n","                  update_rule=update_rule,\n","                  optim_config={\n","                    'learning_rate': learning_rates[update_rule]\n","                  },\n","                  verbose=True)\n","  solvers[update_rule] = solver\n","  solver.train()\n","  print()\n","\n","plt.subplot(3, 1, 1)\n","plt.title('Training loss')\n","plt.xlabel('Iteration')\n","\n","plt.subplot(3, 1, 2)\n","plt.title('Training accuracy')\n","plt.xlabel('Epoch')\n","\n","plt.subplot(3, 1, 3)\n","plt.title('Validation accuracy')\n","plt.xlabel('Epoch')\n","\n","for update_rule, solver in list(solvers.items()):\n","  plt.subplot(3, 1, 1)\n","  plt.plot(solver.loss_history, 'o', label=update_rule)\n","  \n","  plt.subplot(3, 1, 2)\n","  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n","\n","  plt.subplot(3, 1, 3)\n","  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n","  \n","for i in [1, 2, 3]:\n","  plt.subplot(3, 1, i)\n","  plt.legend(loc='upper center', ncol=4)\n","plt.gcf().set_size_inches(15, 15)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zAY4eYeFQnBC","tags":["pdf-inline"]},"source":["## Inline Question 1:\n","\n","AdaGrad, like Adam, is a per-parameter optimization method that uses the following update rule:\n","\n","```\n","cache += dw**2\n","w += - learning_rate * dw / (np.sqrt(cache) + eps)\n","```\n","\n","John notices that when he was training a network with AdaGrad that the updates became very small, and that his network was learning slowly. Using your knowledge of the AdaGrad update rule, why do you think the updates would become very small? Would Adam have the same issue?\n","\n","\n","## Answer: \n","The cache grows by dw^2 each iteration when using AdaGrad so the term \n","dw/(np.sqrt(cache) + eps)\n","converges to 0 causing the model to learn slowly.\n","With Adam the update rule is \n","next_w = w - lr * m_hat / (np.sqrt(v_hat) + eps)\n","where m_hat is a bias corrected moving average of the gredient\n","and v_hat is the bias corrected moving average of the squared gredient\n","that way v_hat doesn't grow as fast as cache grows in comparison to m_hat and dw respectively, and thus Adam won't have the same issue.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K0-OsGpmQnBD"},"source":["# Train a good model!\n","Train the best model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net or a cnn net.\n","\n","You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-FFVdmsQnBD","scrolled":false},"outputs":[],"source":["best_model = None\n","################################################################################\n","# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n","# find batch/layer normalization and dropout useful. Store your best model in  #\n","# the best_model variable.                                                     #\n","################################################################################\n","# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","pass\n","\n","# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","################################################################################\n","#                              END OF YOUR CODE                                #\n","################################################################################"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NrcyXfyDQnBD"},"source":["# Test your model!\n","Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqgyAvntQnBE"},"outputs":[],"source":["y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n","y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n","print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n","print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"FullyConnectedNets.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
