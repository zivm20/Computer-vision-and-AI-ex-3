{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#assert '.'.join(torch.__version__.split('.')[:2]) == '1.4'\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Resize((16,16)),\n",
    "                T.Grayscale()\n",
    "            ])\n",
    "\n",
    "\n",
    "rice_data = dset.ImageFolder('./CV7062610/datasets/rice/Rice_Image_Dataset',\n",
    "                             transform=transform)\n",
    "\n",
    "rice_train, rice_val,rice_test = random_split(rice_data,[0.6,0.25,0.15],torch.Generator().manual_seed(13))\n",
    "\n",
    "loader_train = DataLoader(rice_train, batch_size=64)\n",
    "\n",
    "loader_val = DataLoader(rice_val, batch_size=64)\n",
    "\n",
    "\n",
    "loader_test = DataLoader(rice_test, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#returns an array with the correct nn.Modules for a single model using our architecture\n",
    "def makeModel(in_shape,out_classes,N,K,M,numFilters,filterSizes,poolSizes,hiddenDims,convNorm=False,affineNorm=False,convDrop=0,affineDrop=0):\n",
    "    model = []\n",
    "    #convolution and pooling\n",
    "    for n in range(N):\n",
    "        for k in range(K):\n",
    "            \n",
    "            model.append(nn.Conv2d(in_shape[0],numFilters[n][k],(filterSizes[n][k],filterSizes[n][k]),padding = filterSizes[n][k]//2))\n",
    "            in_shape[0] = numFilters[n][k]\n",
    "            if convNorm:\n",
    "                model.append(nn.BatchNorm2d(in_shape[0]))\n",
    "            model.append(nn.ReLU())\n",
    "            if convDrop > 0:\n",
    "                model.append(nn.Dropout2d(convDrop))\n",
    "        model.append(nn.MaxPool2d(poolSizes[n]))\n",
    "        in_shape[1] = 1 + ((in_shape[1]-poolSizes[n])//poolSizes[n])\n",
    "        in_shape[2] = 1 + ((in_shape[2]-poolSizes[n])//poolSizes[n])\n",
    "        \n",
    "\n",
    "    #prep for affine layers\n",
    "    model.append(Flatten())\n",
    "    in_shape = in_shape[0]*in_shape[1]*in_shape[2]\n",
    "\n",
    "    #affine layers\n",
    "    for m in range(M-1):\n",
    "        \n",
    "        model.append(nn.Linear(in_shape,hiddenDims[m]))\n",
    "        in_shape = hiddenDims[m]\n",
    "        if affineNorm:\n",
    "            model.append(nn.BatchNorm1d(in_shape))\n",
    "        model.append(nn.ReLU())\n",
    "        if affineDrop>0:\n",
    "            model.append(nn.Dropout(affineDrop))\n",
    "\n",
    "\n",
    "    #we always add an extra affine layer\n",
    "    model.append(nn.Linear(in_shape,out_classes))\n",
    "    if affineNorm:\n",
    "        model.append(nn.BatchNorm1d(out_classes))\n",
    "    model.append(nn.ReLU())\n",
    "    if affineDrop>0:\n",
    "        model.append(nn.Dropout(affineDrop))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#returns all the permutations of our paramaters\n",
    "def generate_model_params(param_grid):\n",
    "    \n",
    "    params1 = []\n",
    "    #generate all the possible combinations of number of filters, filter sizes, hidden dims and pool sizes\n",
    "    for N in param_grid[\"N\"]:\n",
    "        for K in param_grid[\"K\"]:\n",
    "            for M in param_grid[\"M\"]:\n",
    "                for numFilters in itertools.product(param_grid[\"numFilters\"], repeat=N*K):\n",
    "                    for filterSizes in itertools.product(param_grid[\"filterSizes\"], repeat=N*K):\n",
    "                        for hiddenDims in itertools.product(param_grid[\"hiddenDims\"], repeat=M):\n",
    "                            for poolSizes in itertools.product(param_grid[\"poolSizes\"], repeat=N):\n",
    "                                \n",
    "                                params = {\n",
    "                                    'N': N,\n",
    "                                    'K': K,\n",
    "                                    'M': M,\n",
    "                                    'numFilters': [[numFilters[i*K + j] for j in range(K)] for i in range(N)],\n",
    "                                    'filterSizes': [[filterSizes[i*K + j] for j in range(K)] for i in range(N)],\n",
    "                                    'poolSizes': list(poolSizes),\n",
    "                                    'hiddenDims': list(hiddenDims),\n",
    "                                    \n",
    "                                }\n",
    "                                \n",
    "                                params1.append(params)\n",
    "\n",
    "    #rest of the params\n",
    "    params = {\n",
    "        'convNorm': param_grid['convNorm'],\n",
    "        'affineNorm': param_grid['affineNorm'],\n",
    "        'convDrop': param_grid['convDrop'],\n",
    "        'affineDrop': param_grid['affineDrop']\n",
    "    }\n",
    "    keys, values = zip(*params.items())\n",
    "    params2 = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "    #combine the 2\n",
    "    param_combinations = []\n",
    "    for param1 in params1:\n",
    "        for param2 in params2:\n",
    "            \n",
    "            param_combinations.append({**param1,**param2})\n",
    "\n",
    "    return param_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar to part34 but returns accuracy\n",
    "def check_accuracy_part5(loader, model,verbose=0):\n",
    "    \n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        if verbose>1:\n",
    "            print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "        return acc\n",
    "    \n",
    "\n",
    "#similar to part34 but returns train and validation accuracy\n",
    "def train_part5(model, optimizer,lossFunction=F.cross_entropy, epochs=1, verbose = 0, print_every=100):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: train accuracy, validation accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = lossFunction(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                if verbose>1:\n",
    "                    print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                    check_accuracy_part5(loader_val, model,verbose=verbose)\n",
    "\n",
    "    train_acc = check_accuracy_part5(loader_train,model,verbose=verbose)\n",
    "    val_acc = check_accuracy_part5(loader_val, model,verbose=verbose)\n",
    "    if verbose > 0:    \n",
    "        print('train loss = %.4f' % (loss.item()))\n",
    "    if verbose == 1:\n",
    "        print('train accuracy: (%.2f), val accuracy: (%.2f)' % (100 * train_acc,100*val_acc))\n",
    "\n",
    "    return train_acc,val_acc\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note, adding paramaters here can greatly increase run time\n",
    "param_grid = {\n",
    "    \n",
    "    'N': [1,2],\n",
    "    'K': [2,3],\n",
    "    'M': [2],\n",
    "    'numFilters': [8],\n",
    "    'filterSizes': [5],\n",
    "    'poolSizes': [2],\n",
    "    'hiddenDims': [64],\n",
    "    'convNorm': [True],\n",
    "    'affineNorm': [True],\n",
    "    'convDrop': [0],\n",
    "    'affineDrop': [0]\n",
    "}\n",
    "params = generate_model_params(param_grid)\n",
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate all the models\n",
    "models = {}\n",
    "for i in range(len(params)):\n",
    "    \n",
    "    models[i]= [makeModel([1,16,16],5,**params[i]),params[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 1, 'K': 2, 'M': 2, 'numFilters': [[8, 8]], 'filterSizes': [[5, 5]], 'poolSizes': [2], 'hiddenDims': [64, 64], 'convNorm': True, 'affineNorm': True, 'convDrop': 0, 'affineDrop': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.3909\n",
      "train accuracy: (95.48), val accuracy: (95.05)\n",
      "--------------------------------------------------------------------\n",
      "{'N': 1, 'K': 3, 'M': 2, 'numFilters': [[8, 8, 8]], 'filterSizes': [[5, 5, 5]], 'poolSizes': [2], 'hiddenDims': [64, 64], 'convNorm': True, 'affineNorm': True, 'convDrop': 0, 'affineDrop': 0}\n",
      "train loss = 0.3152\n",
      "train accuracy: (94.92), val accuracy: (94.49)\n",
      "--------------------------------------------------------------------\n",
      "{'N': 2, 'K': 2, 'M': 2, 'numFilters': [[8, 8], [8, 8]], 'filterSizes': [[5, 5], [5, 5]], 'poolSizes': [2, 2], 'hiddenDims': [64, 64], 'convNorm': True, 'affineNorm': True, 'convDrop': 0, 'affineDrop': 0}\n",
      "train loss = 0.3724\n",
      "train accuracy: (94.05), val accuracy: (93.93)\n",
      "--------------------------------------------------------------------\n",
      "{'N': 2, 'K': 3, 'M': 2, 'numFilters': [[8, 8, 8], [8, 8, 8]], 'filterSizes': [[5, 5, 5], [5, 5, 5]], 'poolSizes': [2, 2], 'hiddenDims': [64, 64], 'convNorm': True, 'affineNorm': True, 'convDrop': 0, 'affineDrop': 0}\n",
      "train loss = 0.3268\n",
      "train accuracy: (92.36), val accuracy: (91.97)\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9575111111111111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "#we can check different learning rates and momentums \n",
    "optimizers = {1:[optim.SGD,{\"lr\":3e-2,\"momentum\":0.9, \"nesterov\":True}]}\n",
    "\n",
    "res = {}\n",
    "best_val = -1\n",
    "best_model = None\n",
    "\n",
    "#iterate over all optimizers and models\n",
    "for k1,net in models.items():\n",
    "    model = nn.Sequential(*net[0])\n",
    "    res[k1] = {}\n",
    "    for k2,opt in optimizers.items():\n",
    "        optimizer = opt[0](model.parameters(),**opt[1])\n",
    "        \n",
    "        print(net[1])\n",
    "        train_acc, val_acc = train_part5(model, optimizer,epochs=2,verbose=1)\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        res[k1][k2] = [val_acc,train_acc,net[1]]\n",
    "        #keep the best one\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_model = model\n",
    "            best_params = net[0]\n",
    "            best_optimizer = optimizer\n",
    "\n",
    "\n",
    "#finally set model and optimizer to be the best ones\n",
    "model = nn.Sequential(*best_params)\n",
    "optimizer = best_optimizer\n",
    "\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "train_part5(model, optimizer, epochs=2)\n",
    "best_model = model\n",
    "check_accuracy_part5(loader_test, best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, \"model2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
